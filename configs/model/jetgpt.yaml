_target_: src.models.gpt.JetGPT

probe_every: 50
vocab_size: 10_000

embed_config:
  hddn_dim: 128
  num_blocks: 2
  norm: LayerNorm
  act_o: SiLU
  act_h: SiLU

encoder_config:
  dim: 512
  ctxt_dim: 64
  num_layers: 6
  num_registers: 1 # This essentially the start-token
  pack_inputs: True # Much faster training but requires half precision

probe_head:
  _target_: mltools.mltools.transformers.ClassAttentionPooling
  _partial_: true
  num_layers: 2
  dim: 128
  layer_config:
    attn_config:
      num_heads: 4
    ff_config:
      mult: 1

optimizer:
  _target_: mltools.mltools.optimisers.AdamWS
  _partial_: True
  lr: 5.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1

scheduler:
  _target_: src.models.utils.linear_warmup_cosine_decay
  _partial_: True
  warmup_steps: 1_000
  total_steps: 200_000
