_target_: src.models.jepa.JetJEPA

probe_every: 50
ema_param_sync: 0.999

embed_config:
  hddn_dim: 128
  num_blocks: 2
  norm: LayerNorm
  act_o: SiLU
  act_h: SiLU
  norm_on_output: True

encoder_config:
  dim: 512
  ctxt_dim: 64
  num_layers: 6
  num_registers: 8
  pack_inputs: True # Much faster training but requires half precision

decoder_config:
  dim: 512 # Unlinke MPM this must be the same as the encoder dim
  num_layers: 2
  pack_inputs: True # Much faster training but requires half precision

probe_head:
  _target_: mltools.mltools.transformers.ClassAttentionPooling
  _partial_: true
  num_layers: 2
  dim: 128
  layer_config:
    attn_config:
      num_heads: 4
    ff_config:
      mult: 1

optimizer:
  _target_: mltools.mltools.optimisers.AdamWS
  _partial_: True
  lr: 5.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1

scheduler:
  _target_: src.models.utils.linear_warmup_cosine_decay
  _partial_: True
  warmup_steps: 1_000
  total_steps: -1 # Will automatically sync to length of dataloader * max_epochs
